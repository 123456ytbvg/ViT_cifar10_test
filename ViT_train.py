from ViT import *
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import os
from tqdm import tqdm
from ç»˜å›¾å°å·¥å…· import *
import matplotlib
matplotlib.use('TkAgg')  # æˆ–è€… 'Qt5Agg'
import matplotlib.pyplot as plt

# æ•°æ®é¢„å¤„ç†
def get_transforms():
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    val_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
    ])

    return train_transform, val_transform


# æ•°æ®åŠ è½½
def get_dataloaders(train_data_path, test_data_path, batch_size=64, num_workers=2):
    train_transform, val_transform = get_transforms()

    try:
        # è®­ç»ƒé›†
        train_dataset = datasets.ImageFolder(
            root=train_data_path,
            transform=train_transform
        )

        # æµ‹è¯•é›†
        test_dataset = datasets.ImageFolder(
            root=test_data_path,
            transform=val_transform
        )

        print(f"âœ… æˆåŠŸåŠ è½½è®­ç»ƒé›†ï¼Œæ€»æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"âœ… æˆåŠŸåŠ è½½æµ‹è¯•é›†ï¼Œæ€»æ ·æœ¬æ•°: {len(test_dataset)}")
        print(f"âœ… ç±»åˆ«: {train_dataset.classes}")

        # åˆ†æè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
        train_class_counts = {}
        for _, label in train_dataset.samples:
            train_class_counts[label] = train_class_counts.get(label, 0) + 1

        print("è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
        for class_idx in sorted(train_class_counts.keys()):
            print(f"  ç±»åˆ« {class_idx}: {train_class_counts[class_idx]} ä¸ªæ ·æœ¬")

        # è®­ç»ƒé›†åˆ†å‰² (90% è®­ç»ƒ, 10% éªŒè¯)
        train_size = int(0.9 * len(train_dataset))
        val_size = len(train_dataset) - train_size
        train_subset, val_subset = torch.utils.data.random_split(
            train_dataset, [train_size, val_size]
        )

        # éªŒè¯é›†ä½¿ç”¨è®­ç»ƒé›†çš„åˆ†å‰²éƒ¨åˆ†
        val_dataset = datasets.ImageFolder(
            root=train_data_path,
            transform=val_transform
        )
        val_subset = torch.utils.data.Subset(val_dataset, val_subset.indices)

        train_loader = DataLoader(
            train_subset, batch_size=batch_size, shuffle=True,
            num_workers=num_workers, pin_memory=True
        )

        val_loader = DataLoader(
            val_subset, batch_size=batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=True
        )

        # æµ‹è¯•é›†ä½¿ç”¨å®Œæ•´çš„å¹³è¡¡æ•°æ®é›†
        test_loader = DataLoader(
            test_dataset, batch_size=batch_size, shuffle=False,
            num_workers=num_workers, pin_memory=True
        )

        return train_loader, val_loader, test_loader, train_dataset.classes

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return None, None, None, None


# è®­ç»ƒå‡½æ•°
def train_epoch(model, dataloader, criterion, optimizer, device, epoch):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(dataloader, desc=f'Epoch {epoch} Training')

    for batch_idx, (inputs, targets) in enumerate(pbar):
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'Loss': f'{running_loss / (batch_idx + 1):.3f}',
            'Acc': f'{100. * correct / total:.2f}%'
        })

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


# éªŒè¯å‡½æ•°
def validate_epoch(model, dataloader, criterion, device, epoch):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    pbar = tqdm(dataloader, desc=f'Epoch {epoch} Validation')

    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(pbar):
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, targets)

            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            pbar.set_postfix({
                'Loss': f'{running_loss / (batch_idx + 1):.3f}',
                'Acc': f'{100. * correct / total:.2f}%'
            })

    epoch_loss = running_loss / len(dataloader)
    epoch_acc = 100. * correct / total

    return epoch_loss, epoch_acc


# å­¦ä¹ ç‡è°ƒåº¦å™¨
def get_scheduler(optimizer, epochs):
    return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)


# ä¿å­˜æ£€æŸ¥ç‚¹
def save_checkpoint(model, optimizer, scheduler, epoch, acc, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'accuracy': acc,
    }, path)

mode = "new"
# mode = "load"
# ä¸»è®­ç»ƒå‡½æ•°
def main():
    # é…ç½®å‚æ•°
    train_data_path = 'CIFAR10_imbalanced/CIFAR10_unbalance'
    test_data_path = 'CIFAR10_balanced/CIFAR10_balance'
    batch_size = 32
    epochs = 500
    learning_rate = 5e-5
    weight_decay = 1e-4
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = Vit(
        patch_size=8,
        embed_dim=384,
        num_heads=6,
        max_seq_length=100,
        encoder_num=6,
        dropout=0.1,
        num_classes=10
    ).to(device)

    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")

    # æ•°æ®åŠ è½½
    train_loader, val_loader, test_loader, classes = get_dataloaders(
        train_data_path, test_data_path, batch_size
    )
    if train_loader is None:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return

    print(f"ğŸ·ï¸  ç±»åˆ«: {classes}")
    print(f"ğŸ“š è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset):,}")
    print(f"ğŸ” éªŒè¯æ ·æœ¬: {len(val_loader.dataset):,}")
    print(f"ğŸ§ª æµ‹è¯•æ ·æœ¬: {len(test_loader.dataset):,}")

    # è®¡ç®—ç±»åˆ«æƒé‡
    def calculate_class_weights(dataloader, num_classes=10):
        class_counts = torch.zeros(num_classes)
        for _, targets in dataloader:
            for target in targets:
                class_counts[target] += 1
        class_counts = torch.clamp(class_counts, min=1)
        weights = 1.0 / class_counts
        weights = weights / weights.sum() * num_classes
        return weights

    class_weights = calculate_class_weights(train_loader).to(device)
    print("âš–ï¸  ç±»åˆ«æƒé‡:", class_weights.cpu().numpy())

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=learning_rate,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    scheduler = get_scheduler(optimizer, epochs)

    # åˆå§‹åŒ–Webå®æ—¶ç»˜å›¾å™¨
    try:
        plotter = TrainingPlotter()
        use_web_plotter = True
        print("ğŸŒ Webå®æ—¶ç»˜å›¾å·²å¯ç”¨!")
        print("ğŸ’¡ è¯·æ‰“å¼€æµè§ˆå™¨æŸ¥çœ‹å®æ—¶è®­ç»ƒè¿›åº¦")
        print("ğŸ“± é€šå¸¸è®¿é—®: http://localhost:8987")
        print("â³ ç­‰å¾…æµè§ˆå™¨è¿æ¥...")
        time.sleep(2)  # ç»™æµè§ˆå™¨ä¸€äº›è¿æ¥æ—¶é—´
    except Exception as e:
        print(f"âŒ Webç»˜å›¾åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ“Š å°†ä½¿ç”¨æ§åˆ¶å°è¾“å‡ºæ¨¡å¼")
        use_web_plotter = False

    # è®­ç»ƒè®°å½•
    best_acc = 0.0

    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ ViT æ¨¡å‹!")
    print("=" * 80)

    try:
        for epoch in range(1, epochs + 1):
            start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device, epoch
            )

            # éªŒè¯
            val_loss, val_acc = validate_epoch(
                model, val_loader, criterion, device, epoch
            )

            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]

            epoch_time = time.time() - start_time

            # æ›´æ–°Webå®æ—¶å›¾è¡¨
            if use_web_plotter:
                plotter.update(epoch, train_loss, val_loss, train_acc, val_acc)

            # è®¡ç®—å‡†ç¡®ç‡å˜åŒ–å’Œå–µå–µæ¶ˆæ¯
            delt = val_acc - best_acc

            # å–µå–µæ¿€åŠ±ç³»ç»Ÿ
            if delt < 0:
                if delt < -0.003:
                    color_msg = "\033[91mæ°”æ­»æˆ‘äº†å–µ ğŸ˜ \033[0m"
                    emoji = "ğŸ’¢"
                else:
                    color_msg = "\033[96mæ€ä¹ˆå›äº‹å–µï¼Ÿ ğŸ¤”\033[0m"
                    emoji = "â“"
            else:
                if delt > 0.003:
                    color_msg = "\033[92må¹²çš„æ¼‚äº®å–µï¼ ğŸ‰\033[0m"
                    emoji = "âœ¨"
                else:
                    color_msg = "\033[93må°±æ˜¯è¿™æ ·å–µ~ ğŸ˜Š\033[0m"
                    emoji = "ğŸ‘"

            # å½©è‰²è¾“å‡º
            print(f'\nâ”Œ{"â”€" * 70}â”')
            print(f'â”‚ ğŸ“… Epoch {epoch:2d}/{epochs} â”‚ â±ï¸  {epoch_time:5.1f}s â”‚ ğŸ“Š LR: {current_lr:.2e} â”‚')
            print(f'â”œ{"â”€" * 70}â”¤')
            print(f'â”‚ ğŸš‚ Train â”‚ Loss: {train_loss:7.4f} â”‚ Acc: {train_acc:6.2f}% â”‚')
            print(f'â”‚ ğŸ§ª Val   â”‚ Loss: {val_loss:7.4f} â”‚ Acc: {val_acc:6.2f}% â”‚ {emoji}')
            print(f'â”‚ {color_msg:^50} â”‚')

            if delt > 0:
                print(f'â”‚ ğŸ¯ å‡†ç¡®ç‡æå‡: +{delt:.3f}%{" ":>30} â”‚')
            elif delt < 0:
                print(f'â”‚ ğŸ“‰ å‡†ç¡®ç‡ä¸‹é™: {delt:.3f}%{" ":>30} â”‚')
            else:
                print(f'â”‚ â¡ï¸  å‡†ç¡®ç‡æŒå¹³{" ":>40} â”‚')
            print(f'â””{"â”€" * 70}â”˜')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                save_checkpoint(
                    model, optimizer, scheduler, epoch, best_acc,
                    'best_vit_model.pth'
                )
                print(f'\nâœ¨ \033[92mğŸŠ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜! å‡†ç¡®ç‡: {best_acc:.2f}%\033[0m\n')

            # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if epoch % 10 == 0:
                save_checkpoint(
                    model, optimizer, scheduler, epoch, val_acc,
                    f'checkpoint_epoch_{epoch}.pth'
                )
                print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: checkpoint_epoch_{epoch}.pth")

    except KeyboardInterrupt:
        print("\nğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
    finally:
        # ä¿å­˜æœ€ç»ˆç»˜å›¾
        if use_web_plotter:
            plotter.save_final_plot()
            plotter.close()
        print(f"\nğŸ“ æ‰€æœ‰è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ°: {os.path.abspath('training_plots')}")

    print(f"\nğŸŠ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")

    # æœ€ç»ˆåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
    print("\nğŸ” åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
    checkpoint = torch.load('best_vit_model.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    final_test_loss, final_test_acc = validate_epoch(
        model, test_loader, criterion, device, "Final Test"
    )

    # æµ‹è¯•ç»“æœè¯„ä»·
    if final_test_acc > 80:
        color = 92
        rating = "ğŸ‰ ä¼˜ç§€!"
    elif final_test_acc > 70:
        color = 93
        rating = "ğŸ‘ è‰¯å¥½"
    elif final_test_acc > 60:
        color = 96
        rating = "âœ… ä¸€èˆ¬"
    else:
        color = 91
        rating = "ğŸ’ª éœ€æ”¹è¿›"

    print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ:")
    print(f"   ğŸ“Š å‡†ç¡®ç‡: \033[{color}m{final_test_acc:.2f}%\033[0m - {rating}")
    print(f"   ğŸ“‰ æŸå¤±: {final_test_loss:.4f}")


if __name__ == "__main__":
    main()